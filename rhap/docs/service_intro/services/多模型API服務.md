# 多模型API服務

## **服務介紹**

多模型API服務包含三層鬆耦合架構中的API代理伺服器（Tier 2）和LLM推論加速器（Tier 3），可供使用者租用及串接。

內建多種開源模型API，專為台灣繁體中文應用進行深度優化，能夠更加理解並處理台灣地區獨特的語言結構、語境和文化背景。除了提升了語言理解的準確度，還能顯著改善文本處理、語音識別和情感分析等領域的應用效果。

透過精準的語言處理能力，企業能夠快速開發出高效且符合本地需求的AI加值服務。


## **服務優點**

- 整合API 代理伺服器（Tier 2）與LLM 推論加速器（Tier 3）
- 提供標準化 API 介面及安全控制機制（Safety Guard），讓使用者可透過 API 租用方式快速建立 LLM 應用服務。
- 提供多種開源模型，適用不同應用場景。
- 針對台灣繁體中文優化，提升語言理解與應用效果。
- 幫助企業快速開發AI加值服務，增強競爭力。

## **架構介紹**

### **2. API 代理伺服器（Tier 2）**

- 建構高效能、低成本、安全可靠的AI開發環境。
- 整合多重安全防護工具，利用Safety Guard Proxy打造堅固的資安防護層，確保模型輸入及輸出資訊皆安全及符合倫理道德。
- 提供輕量化LLM服務，高效能且低資源消耗，讓開發者在有限資源條件下，依然能順暢執行AI推理與應用。
- 內建關聯式資料庫管理系統，支援複雜查詢與大規模應用，確保數據存取與管理的靈活性。
- 整合國網中心計算資源，提供GPU高速運算、GPU容器、CPU虛擬機器、儲存與網路服務，支援AI技術研發與雲端部署。

### **3. LLM 推論加速器（Tier 3）**

- 提供Embedding Engine，專為開源文字嵌入模型的部署與應用設計，支援多模型架構，實現高效嵌入提取，強化生成式AI的準確性與運算效率。
- 支援各式推論框架，透過 GPU 密集運算處理複雜推論需求。未來將持續整合更多開源推論框架及本土non-GPU解決方案，提供更多元推論加速選項。
- 提供TAIDE、Llama、Phi、Mistral、Ministral等開源模型API，並持續更新，確保技術與應用的前瞻性。
 
### **使用情境**
- **智能客服系統**：企業可利用台灣繁體中文優化的多模型API服務，打造一個能夠理解台灣地區特有語言結構的智能客服系統。該系統能自動處理顧客的查詢，提供快速、準確的回應，並能夠識別情感語氣，有效提升顧客滿意度並減少人工客服的工作負擔。
- **醫療文書自動化處理**：醫療機構可以使用多模型API服務來自動化醫療文書的處理，包含病歷錄入、診斷報告生成及醫囑記錄等。該服務特別針對繁體中文進行優化，能高效解析醫學術語及複雜文本，幫助醫療機構提高工作效率、減少錯誤並提升服務品質。
- **電子商務智能推薦系統**：電子商務平台可以利用多模型API服務開發個性化的商品推薦引擎。根據用戶的歷史行為、購物習慣及偏好，系統能快速提供精準的商品推薦，提升用戶體驗並提高轉化率，從而幫助企業在競爭激烈的市場中脫穎而出。



<!--

# API代理伺服器

## LiteLLM

LiteLLM 是一款用來簡化多種大型語言模型（LLM）整合和管理的高效工具。作為開源專案，它提供了一個統一的接口，使開發人員可以方便地存取來自 OpenAI、Azure、Anthropic、Cohere 等多個平台的 LLM。透過 LiteLLM，開發者可以輕鬆地在這些模型間進行互動，無需處理各個 API 的複雜差異，因為它已經將不同模型轉換為一致的 OpenAI 相容格式。

### 什麼是 LiteLLM Proxy？
LiteLLM Proxy 是 LiteLLM 模型輸入/輸出庫中的關鍵元件，充當用戶端應用程式與各種語言模型 API 服務之間的中介軟體。其主要目的是：

- 統一化 ：為 Azure、Anthropic、OpenAI 等多個服務提供統一的 API 格式。
- 簡化複雜性：抽象化不同 API 的細節，提供一致的輸入/輸出格式，減少開發者處理不同模型特性的負擔。

### LiteLLM Proxy 的核心功能
1. 多模型支持
- 廣泛的模型相容性：支持超過 50 種 LLM 模型，包括 Azure、OpenAI、Replicate、Anthropic 等。
- 統一格式：通過單一的格式與多個模型互動，無需為每個模型編寫獨立的API。
2. 一致的輸入/輸出格式
- OpenAI 格式統一：所有模型均使用 OpenAI 的請求和回應格式，文本回應可通過 ['choices'][0]['message']['content'] 取得。
- 簡化數據處理：減少了處理不同模型輸入/輸出格式的複雜性。

# LLM推論加速器

VLLM 和 OLLAMA 位於後端推理引擎層，負責核心推理計算，提供強大的語言生成和語意檢索能力，適合需要高效能和精確推理的應用場景。

## VLLM

### 什麼是 vLLM？
vLLM 是一個開源的大型語言模型（LLM）推理和服務引擎，採用了名為 PagedAttention 的新型記憶體分配演算法。這使得 vLLM 能夠以極高的效率運行模型

### VLLM 的優點
- 卓越的吞吐量：相較於 HuggingFace Transformers（HF），vLLM 的吞吐量提升了 24 倍；相較於 HuggingFace Text Generation Inference（TGI），提升了 3.5 倍。
- 最佳化記憶體使用：傳統系統在 KV-Cache（LLM 記憶體）上會浪費 60%-80%，而 vLLM 將這一浪費降低到 不到 4%，實現了近乎最佳的記憶體利用率。
- 資源節省：由於更高的記憶體效率，vLLM 需要更少的 GPU 就能達到相同的性能，大幅提高了推理速度和成本效益。


## Ollama

### 什麼是 Ollama？
Ollama 是一個旨在簡化在本地電腦上運行開源 LLM 的軟體平台。它消除了管理模型權重、設定和依賴關係的複雜性，使您能夠專注於與 LLM 的互動和探索其功能。

### Ollama 的主要特點

- 本地部署：允許您直接在自己的機器上運行 LLM，提供了對資源的更大控制和資料隱私的保障。

- 專注開源：主要支援開源模型，促進了透明度並允許更高的客製化。

- 簡化的工作流程：Ollama 簡化了運行 LLM 的流程，讓您更容易上手並進行實驗。

- 靈活性：支援各種開源模型，並通過模型文件提供自定義選項，滿足不同的應用需求。

-->