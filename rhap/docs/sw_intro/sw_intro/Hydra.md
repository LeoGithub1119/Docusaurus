<div className="Hydra"></div>

# Hydra

<!--

# 服務介紹

三合一集成服務是 RAP 平台的一體化解決方案，將 前端平台（積木1）、API代理伺服器（積木2） 和 LLM 推論加速器（積木3） 整合在一起，以提供全面的推論處理能力，適合多樣化的企業級應用場景。此服務設計滿足多種推論需求，確保了高效能與靈活性，並通過分層的架構進行最佳化

# 服務優點

運用國網中心算力資源，快速搭建具備GPU環境的私有、獨立與專用的LLM大型語言平台，確保資料安全，讓使用者可建立加值應用服務，無需負擔軟硬體建置與維運成本，適合需要專屬與私有LLM 推論服務的使用者。

-  適合沒有 IT人力或硬體裝置的公司，可以建立專屬和私有的 LLM 推論服務

-  特別是適用於資料不能公開或上傳網路的政府單位或企業，以確保資料安全，提供獨立的 LLM 推論服務

-  無需自備硬體設施，無需購置昂貴的 GPU 裝置，減少投入成本，提供從前端到後端的完整環境

-  高效管理，減少操作繁瑣度，可串接到外部的API Server，或設定串接呼叫自建的 LLM API Server

-  客戶可利用自有資料，把自己訓練或微調過的模型放入本軟體Tier3 服務內，便可使用自己的私有模型

-  易於客戶管理，減少不同系統間整合和管理成本，提供替換進階版的知識庫向量增強檢索專用模型 (Embedding Model) 提升 RAG 能力

-  易於客戶管理，減少不同系統間整合和管理成本


-->




## 前端平台


RAP 平台的前端推論解決方案組包含：  
- AnythingLLM
- OpenWebUI
- Dify
- Streamlit  

它們位於前端平台，負責處理使用者的請求，並提供直觀的圖形化界面。
AnythingLLM與OpenWebUI簡單操作，便於新手使用。
Dify提供了專為政府部門客製化之解決方案。
Streamlit前端則提供整合了RAG系統的解決方案，提供給政府部門客製化應用，及讓一般使用者上傳特定領域資料進行問答。


### AnythingLLM 

AnythingLLM 是最容易使用的多合一 AI 應用程式，它可以執行 RAG、AI Agents 以及更多的功能，而且不需要任何程式碼或基礎架構。


- 零設定、私有化、全方位的 AI 應用：無需繁瑣的開發者設定，提供本地 LLM、RAG 和 AI Agent 的一站式解決方案。
- AI Agents 功能：具備代理（Agent）特性，能夠自動執行一系列任務，提高效率和生產力。
- 完全可客製化：適用於企業或組織，提供與 ChatGPT 相當的完整功能，並具備權限控制，支持任何 LLM、嵌入模型或向量數據庫。
- 無程式碼或基礎架構負擔：使用者無需編寫程式碼或處理複雜的基礎設施，即可享受強大的 AI 功能。

如果想了解 AnythingLLM 的操作，可以參考 [AnythingLLM  使用說明](/docs/sw_intro/tools/AnythingLLM%20使用說明.md)


### OpenWebUI
OpenWebUI 是一個可擴充、功能豐富且易於使用的 AI 介面，設計為完全離線運行。它支持多種大型語言模型（LLM），包括 Ollama 和相容 OpenAI 的 API。其主要特點包括：

- 完全離線運行：無需連線網路即可使用，確保數據的隱私和安全。
- 多樣化的 LLM 支持：相容多種 LLM 運行器，提供靈活的模型選擇，如 Ollama 和 OpenAI 相容的 API。
- 可擴充性：設計為可擴充的架構，允許開發者添加新的功能和擴充軟體，滿足不同的需求。
- 使用者友好：提供直觀的界面和豐富的功能，使得無論是初學者還是專業人士都能輕鬆使用。

如果想了解 OpenWebUI 的操作，可以參考 [Open WebUI 使用說明](/docs/sw_intro/tools/OpenWebUI%20使用說明.md)

### Dify

### Streamlit

## API代理伺服器

### LiteLLM

LiteLLM 是一款用來簡化多種大型語言模型（LLM）整合和管理的高效工具。作為開源專案，它提供了一個統一的接口，使開發人員可以方便地存取來自 OpenAI、Azure、Anthropic、Cohere 等多個平台的 LLM。透過 LiteLLM，開發者可以輕鬆地在這些模型間進行互動，無需處理各個 API 的複雜差異，因為它已經將不同模型轉換為一致的 OpenAI 相容格式。

#### 什麼是 LiteLLM Proxy？
LiteLLM Proxy 是 LiteLLM 模型輸入/輸出庫中的關鍵元件，充當用戶端應用程式與各種語言模型 API 服務之間的中介軟體。其主要目的是：

- 標準化 API ：為 Azure、Anthropic、OpenAI 等多個服務提供統一的 API 格式。
- 簡化複雜性：抽象化不同 API 的細節，提供一致的輸入/輸出格式，減少開發者處理不同模型特性的負擔。
- 附加功能：提供快取、錯誤處理、日誌記錄等附加功能，提升應用的穩定性和可維護性。

#### LiteLLM Proxy 的核心功能
1. 多模型支持
- 廣泛的模型相容性：支持超過 50 種 LLM 模型，包括 Azure、OpenAI、Replicate、Anthropic 等。
- 統一接口：通過單一的接口與多個模型互動，無需為每個模型編寫獨立的呼叫程式碼。
2. 一致的輸入/輸出格式
- OpenAI 格式統一：所有模型均使用 OpenAI 的請求和回應格式，文本回應可通過 ['choices'][0]['message']['content'] 取得。
- 簡化數據處理：減少了處理不同模型輸入/輸出格式的複雜性。




## LLM推論加速器

VLLM 和 OLLAMA 位於後端推理引擎層，負責核心推理計算，提供強大的語言生成和語意檢索能力，適合需要高效能和精確推理的應用場景。

### VLLM

#### 什麼是 vLLM？
vLLM 是一個開源的大型語言模型（LLM）推理和服務引擎，採用了名為 PagedAttention 的新型記憶體分配演算法。這使得 vLLM 能夠以極高的效率運行模型

#### VLLM 的優點
- 卓越的吞吐量：相較於 HuggingFace Transformers（HF），vLLM 的吞吐量提升了 24 倍；相較於 HuggingFace Text Generation Inference（TGI），提升了 3.5 倍。
- 最佳化記憶體使用：傳統系統在 KV-Cache（LLM 記憶體）上會浪費 60%-80%，而 vLLM 將這一浪費降低到 不到 4%，實現了近乎最佳的記憶體利用率。
- 資源節省：由於更高的記憶體效率，vLLM 需要更少的 GPU 就能達到相同的性能，大幅提高了推理速度和成本效益。


### Ollama

#### 什麼是 Ollama？
Ollama 是一個旨在簡化在本地電腦上運行開源 LLM 的軟體平台。它消除了管理模型權重、設定和依賴關係的複雜性，使您能夠專注於與 LLM 的互動和探索其功能。

#### Ollama 的主要特點

- 本地部署：允許您直接在自己的機器上運行 LLM，提供了對資源的更大控制和資料隱私的保障。

- 專注開源：主要支援開源模型，促進了透明度並允許更高的客製化。

- 簡化的工作流程：Ollama 簡化了運行 LLM 的流程，讓您更容易上手並進行實驗。

- 靈活性：支援各種開源模型，並通過模型文件提供自定義選項，滿足不同的應用需求。



## 模型調整

如果想要更換 AnythingLLM 或 OpenWebUI 上的對話的模型
<br />

可以參考 [AnythingLLM 的模型設定](/docs/sw_intro/tools/AnythingLLM%20使用說明.md#模型設定) 跟 [OpenWebUI 的模型設定](/docs/sw_intro/tools/OpenWebUI%20使用說明.md#模型設定)
